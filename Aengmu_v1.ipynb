{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aengmu_v1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPhBOChMdmAsVWFQzwQOMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/Aengmu/blob/main/Aengmu_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtLYgO_OxLaU"
      },
      "source": [
        "# [ERROR 4] Korean(Hangul) breaking phenomenon Solution on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce18aOPbxLkY"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 4. plot \"Korean\" breaking phenomenon\n",
        "- Solution: Installing(↓) and setting(plt.rc('font', family='NanumBarunGothic')) Nanum font, after runtime restart\n",
        "\"\"\"\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96hYYUFzcaxY"
      },
      "source": [
        "# Google Drive Local Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHqDHk2ucc7M"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgHeq1Zcb7M2"
      },
      "source": [
        "# Install Morpheme analyzer\n",
        "*   [Reference](https://soohee410.github.io/compare_tagger)\n",
        "*   [Install](https://sanghyu.tistory.com/170)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtYcWqVsb5Xk"
      },
      "source": [
        "# okt, komoran, kkma\n",
        "# install konlpy (okt, komoran, kkma)\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy\n",
        "\n",
        "# mecab (take a long time)\n",
        "# set env\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# install konlpy (mecab)\n",
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GobPxVERuegZ"
      },
      "source": [
        "# Error Solution\n",
        "\n",
        "1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings' [(JVM)](https://gyulogs.tistory.com/130)\n",
        "2. NameError: name 'Tagger' is not defined [(Mecab)](https://sosomemo.tistory.com/31)\n",
        "3. ParserError: Error tokenizing data. C error [(Pandas)](https://mskim8717.tistory.com/82)\n",
        "4. plot \"Korean\" breaking phenomenon on Colab [(Matplotlib)](https://teddylee777.github.io/colab/colab-korean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpM2dDEuuesD"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings'\n",
        "- Solution: /usr/local/lib/python3.7/dist-packages/konlpy/jvm.py, 67 line (convertStrings=True) comments processing and save jvm.py before import pakage\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "ERROR 2. NameError: name 'Tagger' is not defined \n",
        "- Solution: Execute mecab.sh script (under code excute)\n",
        "\"\"\"\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu59D-1sc93c"
      },
      "source": [
        "# Import\n",
        "*   [KoNLPy Reperence](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRaXnlVOWpot"
      },
      "source": [
        "# import for MechineLearning\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import metrics, losses, callbacks\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# import Morpheme analyzer\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "from konlpy.utils import pprint\n",
        "\n",
        "# import etc\n",
        "from google.colab import files\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Change run location\n",
        "%cd /content/drive/My Drive/DeepLearning/PROJECT_AON\n",
        "# Matplotlib set font on NanumBarunGothic\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "# Numpy print set\n",
        "np.set_printoptions(linewidth=200, precision=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGm7Up9L5Y98"
      },
      "source": [
        "#Grobal Variable\n",
        "*   [Concept Reference](https://reniew.github.io/25/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4l77b8v5aKe"
      },
      "source": [
        "\"\"\"\n",
        "ranking about time spent morpheme analyzing (The fastest is number one.)\n",
        "\n",
        "1. Mecab\n",
        "2. Komoran\n",
        "3. Okt\n",
        "4. Kkma\n",
        "\n",
        "Mecab is faster than Kkma about 30~40 times\n",
        "Mecab is faster than Okt about 10 times\n",
        "Mecab is faster than Komoran about 5 times\n",
        "\"\"\"\n",
        "\n",
        "# Early versions use macab.\n",
        "MECAB = Mecab()\n",
        "\n",
        "# Learning target's name\n",
        "AI_TARGET_NAME = \"대석\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtyggGY871Bm"
      },
      "source": [
        "# Funtion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm5-VhoeT8Jx"
      },
      "source": [
        "Data Prepare Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8emd9lpUyXR"
      },
      "source": [
        "# Extract reply time (minute)\n",
        "def extract_replytime(content) :\n",
        "    idx_list1 = [i for i, value in enumerate(content) if value == ']']\n",
        "    idx_list2 = [i for i, value in enumerate(content) if value == '[']\n",
        "\n",
        "    end_idx = idx_list1[1]\n",
        "    start_idx = idx_list2[1] + 1\n",
        "\n",
        "    replytime = content[start_idx : end_idx]\n",
        "\n",
        "    if replytime[:2] == \"오전\" :\n",
        "        replytime = (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "    else :\n",
        "        replytime = 60 * 12 + (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "\n",
        "    return replytime\n",
        "\n",
        "# Comparison about reply time\n",
        "def compare_replytime(pre_text, cur_text) :\n",
        "    pre_time = extract_replytime(pre_text)\n",
        "    cur_time = extract_replytime(cur_text)\n",
        "\n",
        "    result = False\n",
        "    if cur_time - pre_time >= 10 : result = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Extract Data about kakao talk's content\n",
        "def extract_data(X_dataset, y_dataset, talk_list) :\n",
        "    global AI_TARGET_NAME\n",
        "\n",
        "    before_respondent = \"\"\n",
        "    for talk in talk_list :\n",
        "        for texts in talk.values :\n",
        "            tmp_X_texts = []\n",
        "            tmp_Y_texts = []\n",
        "            texts = texts.tolist()\n",
        "            for idx, text in enumerate(texts) : \n",
        "                # Don't extract email and date\n",
        "                if (not \"저장한 날짜\" in text) and (not \"--------------\" in text) and (not \"@\" in text) :\n",
        "                    if not \"]\" in text : \n",
        "                        if before_respondent == AI_TARGET_NAME : tmp_Y_texts.append(text)\n",
        "                        else : tmp_X_texts.append(text)\n",
        "\n",
        "                        continue\n",
        "                    else : cur_respondent = text[1:text.index(\"]\")]\n",
        "\n",
        "                    if (\":\" in text) and (before_respondent != \"\") :\n",
        "                        if compare_replytime(texts[idx - 1], text) or cur_respondent != before_respondent:\n",
        "                            if len(tmp_X_texts) != 0 : X_dataset.append(tmp_X_texts)\n",
        "                            if len(tmp_Y_texts) != 0 : y_dataset.append(tmp_Y_texts)\n",
        "                            tmp_X_texts = []\n",
        "                            tmp_Y_texts = []\n",
        "\n",
        "                    if AI_TARGET_NAME in text :\n",
        "                        ptext = text[(text.rfind(']') + 2) : len(text)]\n",
        "                        if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                            before_respondent = AI_TARGET_NAME\n",
        "                            if ptext != '' : tmp_Y_texts.append(ptext)\n",
        "                    elif \"]\" in text :\n",
        "                        ptext = text[(text.rfind(']') + 2) : len(text)]\n",
        "                        if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                            before_respondent = cur_respondent\n",
        "                            if ptext != '' : tmp_X_texts.append(ptext)\n",
        "\n",
        "                    if idx == len(texts) - 1 :\n",
        "                        if len(tmp_X_texts) != 0 : X_dataset.append(tmp_X_texts)\n",
        "                        if len(tmp_Y_texts) != 0 : y_dataset.append(tmp_Y_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smqf-vLlU03k"
      },
      "source": [
        "Data Pre-processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMp3NxIaU1Cp"
      },
      "source": [
        "# Auto calibrate spacing functions\n",
        "def auto_spacing(X_dataset, y_dataset) :\n",
        "    calibrate_spcaing(X_dataset)\n",
        "    calibrate_spcaing(y_dataset)\n",
        "\n",
        "def calibrate_spcaing(context_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    for idx, texts in enumerate(context_dataset) :\n",
        "        for repairIdx, text in enumerate(texts) :\n",
        "            analyzedRes = MECAB.pos(text)\n",
        "            for mecabR in analyzedRes :\n",
        "                # \"MAG\" == Adverb\n",
        "                if mecabR[1] == \"MAG\" :\n",
        "                    try :\n",
        "                        startIdx = text.index(mecabR[0])\n",
        "                        if len(mecabR[0]) == 1 :\n",
        "                            if text[startIdx + 1] != \" \" and text[startIdx + 1].isalnum() : \n",
        "                                if text[startIdx + 2] != \" \" :\n",
        "                                    text = text[:startIdx] + text[startIdx] + \" \" + text[startIdx + 1 :]\n",
        "                                    context_dataset[idx][repairIdx] = text\n",
        "                        else : \n",
        "                            if text[startIdx + len(mecabR[0])] != \" \" and text[startIdx + len(mecabR[0])].isalnum() : \n",
        "                                if text[startIdx + len(mecabR[0]) + 1] != \" \" :\n",
        "                                    text = text[:startIdx] + text[startIdx : startIdx + len(mecabR[0])] + \" \" + text[startIdx + len(mecabR[0]) :]\n",
        "                                    context_dataset[idx][repairIdx] = text\n",
        "                    except IndexError :\n",
        "                        continue\n",
        "\n",
        "# Extraction 1 word by 1 content (use in Early version Model)\n",
        "def extract_1word(X_dataset, y_dataset) :\n",
        "    \"\"\" If don't use list, data type is str. this is caused by need just one word on word2vec \"\"\"\n",
        "    for idx in range(0, len(X_dataset)) : X_dataset[idx] = X_dataset[idx][0].split()\n",
        "    for idx in range(0, len(y_dataset)) : y_dataset[idx] = y_dataset[idx][0].split()\n",
        "\n",
        "# Delete stopword in sentence\n",
        "def stopword_filtering(X_dataset, y_dataset) :\n",
        "    delete_stopword(X_dataset)\n",
        "    delete_stopword(y_dataset)\n",
        "\n",
        "def delete_stopword(word_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "    for idx in range(0, len(word_dataset)) :\n",
        "        #print(word_dataset[idx])\n",
        "        tokenized_data = MECAB.morphs(word_dataset[idx][0])\n",
        "        tokenized_data = [word for word in tokenized_data if not word in stopwords]\n",
        "        word_dataset[idx] = tokenized_data\n",
        "\n",
        "# If x and y size different, equalize that\n",
        "def equalize_size(X_dataset, y_dataset) :\n",
        "    if len(X_dataset) > len(y_dataset) : X_dataset = X_dataset[1 : len(y_dataset) + 1]\n",
        "    elif len(y_dataset) > len(X_dataset) : y_dataset = y_dataset[1 : len(X_dataset) + 1]\n",
        "\n",
        "    return X_dataset, y_dataset\n",
        "    \n",
        "# Reshaping size on ndarray for training\n",
        "def reshape_sizing(X_dataset, y_dataset) :\n",
        "    X_dataset = X_dataset.reshape(X_sizing(X_dataset))\n",
        "    y_dataset = y_dataset.reshape(y_sizing(y_dataset))\n",
        "\n",
        "    return X_dataset, y_dataset\n",
        "\n",
        "def X_sizing(X_dataset) :\n",
        "    common_factors = factorization(X_dataset.shape[0]) # 공약수\n",
        "    step_size = common_factors[0] # timesteps\n",
        "    vector_length = common_factors[1] # n-length vectors\n",
        "\n",
        "    return int((X_dataset.shape[0] / step_size) / vector_length), step_size, vector_length\n",
        "\n",
        "def y_sizing(y_dataset) :\n",
        "    common_factors = factorization(y_dataset.shape[0])\n",
        "    dim_2 = common_factors[0] * common_factors[1]\n",
        "\n",
        "    return int(y_dataset.shape[0] / dim_2), dim_2\n",
        "\n",
        "# Factorization for reshaping's variable\n",
        "def factorization(x) :\n",
        "    common_factors = []\n",
        "    d = 2 \n",
        "    \n",
        "    while d <= x: \n",
        "        if x % d == 0: \n",
        "            common_factors.append(d)\n",
        "            x = x / d \n",
        "        else: d = d + 1\n",
        "    \n",
        "    return common_factors\n",
        "\n",
        "# reshape_sizing 메서드 수정해야함 (하는 중)\n",
        "# 빈도수 너무 높거나 낮은건 제외하는 메서드 추가해야됌 (다음에 할 거)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lglCQxR0WGhE"
      },
      "source": [
        "Word2Vec Functions (Distributed Representation)\n",
        "*   [Word2Vec Reference 1](https://ebbnflow.tistory.com/153)\n",
        "*   [Word2Vec Reference 2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)\n",
        "*   [PCA(sklearn) Reference](https://m.blog.naver.com/tjdrud1323/221720259834)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaczn0GQ71N4"
      },
      "source": [
        "# Draw a two-dimensional graph by entering the words, values of the two-dimensional X-axis, and values of the Y-axis.\n",
        "def plot_2d_graph(vocabs, xs, ys):\n",
        "    plt.figure(figsize=(8 ,6))\n",
        "    plt.scatter(xs, ys, marker = 'o')\n",
        "    for i, v in enumerate(vocabs):\n",
        "        plt.annotate(v, xy=(xs[i], ys[i]))\n",
        "\n",
        "# Conversioning dataset to word2vec format\n",
        "def dataset_to_word2vec(X_dataset, y_dataset) :\n",
        "    return word2vec(X_dataset, 'X'), word2vec(y_dataset, 'y')\n",
        "\n",
        "def word2vec(word_dataset, T) :\n",
        "    # Init words and vectors\n",
        "    w2v = Word2Vec(word_dataset, size=100, window=3, min_count=1, workers=6, sg=1)\n",
        "\n",
        "    # Set word vectors\n",
        "    word_vectors = w2v.wv\n",
        "    vocabs = word_vectors.vocab.keys()\n",
        "    word_vector_list = [word_vectors[v] for v in vocabs]\n",
        "\n",
        "    # Confirm word similarity\n",
        "    # if \"경호\" in vocabs : print(word_vectors.most_similar(\"경호\"))\n",
        "    # else : print(word_vectors.most_similar(\"대석\"))\n",
        "    \n",
        "    pca = PCA(n_components=2)\n",
        "    xys = pca.fit_transform(word_vector_list)\n",
        "    xs = xys[:,0]\n",
        "    ys = xys[:,1]\n",
        "\n",
        "    # plot_2d_graph(vocabs, xs, ys)\n",
        "    return xys.reshape(-1)\n",
        "\n",
        "# vec2word 함수 만들어야 함 (다다음에 할 거)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxjmmKaVlO9p"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea9DIOdiQ_0o"
      },
      "source": [
        "Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcHWaWjaREpT"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 3. ParserError: Error tokenizing data. C error \n",
        "- Solution: add code in read_csv = , sep='\\t'\n",
        "\"\"\"\n",
        "# uploaded_files = files.upload()\n",
        "\n",
        "file_list = os.listdir('./')\n",
        "fs = [f for f in file_list if f.endswith('.txt')]\n",
        "\n",
        "talk_list = []\n",
        "for f in fs :\n",
        "    talk_list.append(pd.read_csv(f, sep='\\t'))\n",
        "\n",
        "# Init Dataset\n",
        "X_dataset = []\n",
        "y_dataset = []\n",
        "\n",
        "# Data extraction\n",
        "extract_data(X_dataset, y_dataset, talk_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG9R5wKTSNwi"
      },
      "source": [
        "Pre-processing Data\n",
        "*  [Dataset Classification Reference 1](https://ganghee-lee.tistory.com/38)\n",
        "*  [Dataset Classification Reference 2](https://ysyblog.tistory.com/69)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HENnALQjSN3l"
      },
      "source": [
        "# Space Modification\n",
        "auto_spacing(X_dataset, y_dataset)\n",
        "\n",
        "# Early version is used y_list[][0] and x_list[][0] (1 word) by learning model\n",
        "extract_1word(X_dataset, y_dataset)\n",
        "\n",
        "# Stopword deleted\n",
        "stopword_filtering(X_dataset, y_dataset)\n",
        "\n",
        "# Size equalize\n",
        "X_dataset, y_dataset = equalize_size(X_dataset, y_dataset)\n",
        "\n",
        "# Data type change on np.ndarray\n",
        "X_dataset = np.asarray(X_dataset).astype(object)\n",
        "y_dataset = np.asarray(y_dataset).astype(object)\n",
        "\n",
        "# Word2Vec on dataset\n",
        "X_dataset, y_dataset = dataset_to_word2vec(X_dataset, y_dataset)\n",
        "\n",
        "# Dataset reshape\n",
        "X_dataset, y_dataset = reshape_sizing(X_dataset, y_dataset)\n",
        "\n",
        "# Data division (Train : Validation : Test = 6 : 2 : 2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset,\n",
        "                                                    test_size = 0.2)\n",
        "print(\"########## Train + Validation (X,) (y,) / Test (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                  test_size = 0.2)\n",
        "print(\"########## Train (X,) (y,) / Validation (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQOW7kTfTKEw"
      },
      "source": [
        "Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liOq3dcjTKLp"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same', input_shape=(X_train.shape[1:])))\n",
        "model.add(MaxPooling1D(pool_size=2, paddig='same'))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2, padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation=\"relu\")) # Performance improves once it passes through layers that have been amplified datas\n",
        "model.add(Dense(1, activation=\"softmax\")) # output data == n( == y_dataset.shape[1]) things\n",
        "model.compile(\n",
        "    loss=losses.CategoricalCrossentropy(), # loss function for NLP is categorical_crossentropy(keras.losses) << personally\n",
        "    optimizer=Adam(), # pram ex. learning_rate=0.0001\n",
        "    metrics=[metrics.CategoricalAccuracy()] # accuracy for NLP is categorical_accuracy(keras.metrics) << personally\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMh-ndV-TQI9"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrBXhqQVTQPY"
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "          validation_data = (X_val, y_val),\n",
        "          batch_size = 4, # Big == good (but enough)\n",
        "          epochs = 80,\n",
        "          callbacks=[ # helper for learning\n",
        "                     callbacks.EarlyStopping(monitor='binary_accuracy', mode='max', verbose=1, patience=5, baseline=0.6) # stop when monitor is satisfact mode\n",
        "                     # callbacks.ModelCheckpoint('best_model.h5', monitor='binary_accuracy', mode='max', save_best_only=True) # optimal pram save\n",
        "          ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmS61Q2YTW0u"
      },
      "source": [
        "Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXUaZL-dTW84"
      },
      "source": [
        "# Acc and Loss print\n",
        "learning_lost, learning_acc = model.evaluate(X_train, y_train, verbose=2)\n",
        "print(\"Learning accuracy :\", learning_acc)\n",
        "print(\"Learning loss % :\", learning_lost)\n",
        "\n",
        "# loss and acc graph (train and val)\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df[[\"loss\", \"val_loss\"]].plot()\n",
        "history_df[[\"categorical_accuracy\", \"val_categorical_accuracy\"]].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA4mqS5DTpn5"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3zCKMclQz2"
      },
      "source": [
        "Aengmu = model.predict(X_test)\n",
        "print(Aengmu[0])\n",
        "print(y_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}