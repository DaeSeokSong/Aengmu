{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-traning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAUjk4UQfnzxus2Ki5GM51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/NLP-Aengmu/blob/main/NLP_traning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212G1X3KEeuk"
      },
      "source": [
        "# **NLP training**\n",
        "\n",
        "This is the code that I am personally working on by referring to the [Github - microsoft/nlp-recipes](https://github.com/microsoft/nlp-recipes) that based on MIT license.\n",
        "However, I do not intend to write the code for commercial purposes and will only write it for personal learning purposes.\n",
        "\n",
        "Since the codewriter is Korean, some annotations can be included in Korean for convenience when studying.\n",
        "\n",
        "Alternatively, I can use a translator called [Papago](https://papago.naver.com/) to write down inaccurate interpretations of the original text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWUjJlSgFx6l"
      },
      "source": [
        "# **1. Embeddings**\n",
        "### Developing Word Embeddings\n",
        "\n",
        "유사성 baseline_deep_dive 노트북처럼 사전 훈련된 임베딩을 사용하는 대신 자체 데이터 집합을 사용하여 단어 임베딩을 학습시킬 수 있습니다.\n",
        "이 노트북에서는 단어 2vec, GloVe 및 fastText 모델을 사용하여 단어 임베딩을 생성하는 교육 과정을 시연합니다. \n",
        "\n",
        "이 작업에는 STS 벤치마크 데이터 세트를 활용하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gscl25z_ILeI"
      },
      "source": [
        "### *Import and Preparing Dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WyWo--7KDwF"
      },
      "source": [
        "Microsoft 에서 제공하는 nlp-recipes Repository의 [utils_nlp](https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp) 라이브러리 가져오는 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnO6raYSJh5s"
      },
      "source": [
        "!pip install -e git+https://github.com/microsoft/nlp-recipes.git@master#egg=utils_nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I3NXKr_8MgD"
      },
      "source": [
        "import gensim\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Set the environment path\n",
        "sys.path.append(\"../..\")\n",
        "\n",
        "import numpy as np\n",
        "from utils_nlp.dataset.preprocess import (\n",
        "    to_lowercase, # 각 DataFrame의 요소들의 값이 str(문자열)이면 해당 요소의 값을 모두 소문자로 만드는 메서드\n",
        "    to_spacy_tokens, # 토큰 추출\n",
        "    rm_spacy_stopwords, # Stopwords 제거\n",
        ")\n",
        "from utils_nlp.dataset import stsbenchmark\n",
        "from utils_nlp.common.timer import Timer\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.fasttext import FastText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXKHBCPOIVcS"
      },
      "source": [
        "# Set the path for where your repo is located\n",
        "NLP_REPO_PATH = os.path.join('..','..')\n",
        "\n",
        "# Set the path for where your datasets are located\n",
        "BASE_DATA_PATH = os.path.join(NLP_REPO_PATH, \"data\")\n",
        "\n",
        "# Set the path for location to save embeddings\n",
        "SAVE_FILES_PATH = os.path.join(BASE_DATA_PATH, \"trained_word_embeddings\")\n",
        "if not os.path.exists(SAVE_FILES_PATH):\n",
        "    os.makedirs(SAVE_FILES_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PobQ3k1RIaPX"
      },
      "source": [
        "### *Load and Preprocess Data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O77KHFghIWI2"
      },
      "source": [
        "# Produce a pandas dataframe for the training set\n",
        "train_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
        "\n",
        "# Clean the sts dataset\n",
        "sts_train = stsbenchmark.clean_sts(train_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NVhjk0NIfAl"
      },
      "source": [
        "sts_train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy1kELiIIgyv"
      },
      "source": [
        "# Check the size of our dataframe\n",
        "sts_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bYegeT2I5li"
      },
      "source": [
        "### *Training set preprocessing*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989zj87qI691"
      },
      "source": [
        "# Convert all text to lowercase\n",
        "df_low = to_lowercase(sts_train)  \n",
        "# Tokenize text\n",
        "sts_tokenize = to_spacy_tokens(df_low) \n",
        "# Tokenize with removal of stopwords\n",
        "sts_train_stop = rm_spacy_stopwords(sts_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoaSQhb5I9LR"
      },
      "source": [
        "# Append together the two sentence columns to get a list of all tokenized sentences.\n",
        "all_sentences =  sts_train_stop[[\"sentence1_tokens_rm_stopwords\", \"sentence2_tokens_rm_stopwords\"]]\n",
        "# Flatten two columns into one list and remove all sentences that are size 0 after tokenization and stop word removal.\n",
        "sentences = [i for i in all_sentences.values.flatten().tolist() if len(i) > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9n5wzQ5I_e5"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ1utb87JCu5"
      },
      "source": [
        "sentence_lengths = [len(i) for i in sentences]\n",
        "print(\"Minimum sentence length is {} tokens\".format(min(sentence_lengths)))\n",
        "print(\"Maximum sentence length is {} tokens\".format(max(sentence_lengths)))\n",
        "print(\"Median sentence length is {} tokens\".format(np.median(sentence_lengths)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUD71eU8JDE5"
      },
      "source": [
        "sentences[:10]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}