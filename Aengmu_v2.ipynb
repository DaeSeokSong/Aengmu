{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aengmu_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjx8uvCysamuL1P+HiRUuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/NLP-Aengmu/blob/main/Aengmu_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtLYgO_OxLaU"
      },
      "source": [
        "# [ERROR 4] Korean(Hangul) breaking phenomenon Solution on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce18aOPbxLkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a968c0d3-2bf8-4045-d3e8-9870172f8b55"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 4. plot \"Korean\" breaking phenomenon\n",
        "- Solution: Installing(↓) and setting(plt.rc('font', family='NanumBarunGothic')) Nanum font, after runtime restart\n",
        "\"\"\"\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 70 not upgraded.\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 4 dirs\n",
            "/usr/share/fonts/truetype/dejavu: caching, new cache contents: 22 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96hYYUFzcaxY"
      },
      "source": [
        "# Google Drive Local Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHqDHk2ucc7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be24df71-8e91-412b-f448-931d970a4aca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgHeq1Zcb7M2"
      },
      "source": [
        "# Install Morpheme analyzer\n",
        "*   [Reference](https://soohee410.github.io/compare_tagger)\n",
        "*   [Install](https://sanghyu.tistory.com/170)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtYcWqVsb5Xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc15ef6-fc65-457b-ab04-eff5eda7e5fb"
      },
      "source": [
        "# okt, komoran, kkma\n",
        "# install konlpy (okt, komoran, kkma)\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy\n",
        "\n",
        "# mecab (take a long time)\n",
        "# set env\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# install konlpy (mecab)\n",
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "python3-dev is already the newest version (3.6.7-1~18.04).\n",
            "openjdk-8-jdk is already the newest version (8u292-b10-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 70 not upgraded.\n",
            "Requirement already satisfied: JPype1 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1) (3.7.4.3)\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "mecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n",
            "Processing /tmp/mecab-python-0.996\n",
            "Building wheels for collected packages: mecab-python\n",
            "  Building wheel for mecab-python (setup.py): started\n",
            "  Building wheel for mecab-python (setup.py): finished with status 'done'\n",
            "  Created wheel for mecab-python: filename=mecab_python-0.996_ko_0.9.2-cp37-cp37m-linux_x86_64.whl size=141809 sha256=b9c84783ca295b332ac20758406094f8d7fa9118b09ead3611c5c51c4c2832e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/7b/9f/2922869bef86c3354ae7034f7a3647c573ee1997c2dad0290a\n",
            "Failed to build mecab-python\n",
            "Installing collected packages: mecab-python\n",
            "  Attempting uninstall: mecab-python\n",
            "    Found existing installation: mecab-python 0.996-ko-0.9.2\n",
            "    Uninstalling mecab-python-0.996-ko-0.9.2:\n",
            "      Successfully uninstalled mecab-python-0.996-ko-0.9.2\n",
            "    Running setup.py install for mecab-python: started\n",
            "    Running setup.py install for mecab-python: finished with status 'done'\n",
            "Successfully installed mecab-python-0.996-ko-0.9.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 8: fg: no job control\n",
            "bash: line 11: fg: no job control\n",
            "  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n",
            "  WARNING: Built wheel for mecab-python is invalid: Metadata 1.2 mandates PEP 440 version, but '0.996-ko-0.9.2' is not\n",
            "  DEPRECATION: mecab-python was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GobPxVERuegZ"
      },
      "source": [
        "# Error Solution\n",
        "\n",
        "1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings' [(JVM)](https://gyulogs.tistory.com/130)\n",
        "2. NameError: name 'Tagger' is not defined [(Mecab)](https://sosomemo.tistory.com/31)\n",
        "3. ParserError: Error tokenizing data. C error [(Pandas)](https://mskim8717.tistory.com/82)\n",
        "4. plot \"Korean\" breaking phenomenon on Colab [(Matplotlib)](https://teddylee777.github.io/colab/colab-korean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpM2dDEuuesD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b529b0-39fe-44a5-d7d4-d253c4ff4466"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings'\n",
        "- Solution: /usr/local/lib/python3.7/dist-packages/konlpy/jvm.py, 67 line (convertStrings=True) comments processing and save jvm.py before import pakage\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "ERROR 2. NameError: name 'Tagger' is not defined \n",
        "- Solution: Execute mecab.sh script (under code excute)\n",
        "\"\"\"\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "openjdk-8-jdk is already the newest version (8u292-b10-0ubuntu1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 70 not upgraded.\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: JPype1-py3 in /usr/local/lib/python3.7/dist-packages (0.5.5.4)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "mecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu59D-1sc93c"
      },
      "source": [
        "# Import\n",
        "*   [KoNLPy Reperence](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRaXnlVOWpot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c95ac9-c193-4f44-c322-4a91205771e6"
      },
      "source": [
        "# import for MechineLearning\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import metrics, losses, callbacks\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad, SGD\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
        "\n",
        "# import Morpheme analyzer\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "from konlpy.utils import pprint\n",
        "\n",
        "# import etc\n",
        "from google.colab import files\n",
        "import time\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "# Change run location, 제출시에 제거\n",
        "%cd /content/drive/My Drive/DeepLearning/PROJECT_AON\n",
        "# Matplotlib set font on NanumBarunGothic\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "# Numpy print set\n",
        "np.set_printoptions(linewidth=200, precision=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/DeepLearning/PROJECT_AON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGm7Up9L5Y98"
      },
      "source": [
        "#Grobal Variable\n",
        "*   [Concept Reference](https://reniew.github.io/25/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4l77b8v5aKe"
      },
      "source": [
        "\"\"\"\n",
        "ranking about time spent morpheme analyzing (The fastest is number one.)\n",
        "\n",
        "1. Mecab\n",
        "2. Komoran\n",
        "3. Okt\n",
        "4. Kkma\n",
        "\n",
        "Mecab is faster than Kkma about 30~40 times\n",
        "Mecab is faster than Okt about 10 times\n",
        "Mecab is faster than Komoran about 5 times\n",
        "\"\"\"\n",
        "\n",
        "# Early versions use macab.\n",
        "MECAB = Mecab()\n",
        "\n",
        "# Learning target's name\n",
        "AI_TARGET_NAME = \"대석\"\n",
        "\n",
        "# Output size Or Time_step\n",
        "OOT = 5\n",
        "\n",
        "# Word2vec model\n",
        "CORPUS = Word2VecKeyedVectors(vector_size=OOT)\n",
        "\n",
        "# Dataset number\n",
        "DN = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtyggGY871Bm"
      },
      "source": [
        "# Funtion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm5-VhoeT8Jx"
      },
      "source": [
        "Data Prepare Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8emd9lpUyXR"
      },
      "source": [
        "# Functions that extract reply time (minute)\n",
        "def extract_replytime(content) :\n",
        "    idx_list1 = [i for i, value in enumerate(content) if value == ']']\n",
        "    idx_list2 = [i for i, value in enumerate(content) if value == '[']\n",
        "\n",
        "    end_idx = idx_list1[1]\n",
        "    start_idx = idx_list2[1] + 1\n",
        "\n",
        "    replytime = content[start_idx : end_idx]\n",
        "\n",
        "    if replytime[:2] == \"오전\" :\n",
        "        replytime = (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "    else :\n",
        "        replytime = 60 * 12 + (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "\n",
        "    return replytime\n",
        "\n",
        "# Functions that comparison about reply time\n",
        "def compare_replytime(pre_text, cur_text) :\n",
        "    pre_time = extract_replytime(pre_text)\n",
        "    cur_time = extract_replytime(cur_text)\n",
        "\n",
        "    result = False\n",
        "    if cur_time - pre_time >= 10 : result = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Functions that extract Data about kakao talk's content\n",
        "def extract_data(X_dataset, y_dataset, talk_list) :\n",
        "    global AI_TARGET_NAME\n",
        "\n",
        "    for talk in talk_list :\n",
        "        tmp_X_respondents = []\n",
        "        tmp_y_respondents = []\n",
        "        for texts in talk.values :\n",
        "            tmp_X_replys = []\n",
        "            tmp_y_replys = []\n",
        "\n",
        "            tmp_X_texts = []\n",
        "            tmp_y_texts = []\n",
        "\n",
        "            before_respondent = \"\"\n",
        "\n",
        "            texts = texts.tolist()\n",
        "            for idx, text in enumerate(texts) : \n",
        "                # Don't extract email and date\n",
        "                if (not \"저장한 날짜\" in text) and (not \"--------------\" in text) and (not \"@\" in text) :\n",
        "                    if not \"]\" in text : \n",
        "                        if before_respondent == AI_TARGET_NAME : tmp_y_texts.append(text)\n",
        "                        else : tmp_X_texts.append(text)\n",
        "\n",
        "                        continue\n",
        "                    else : cur_respondent = text[1:text.index(\"]\")]\n",
        "\n",
        "                    if (\":\" in text) and (before_respondent != \"\") :\n",
        "                        if compare_replytime(texts[idx - 1], text) or cur_respondent != before_respondent:\n",
        "                            if len(tmp_X_texts) != 0 : tmp_X_replys.append(tmp_X_texts)\n",
        "                            if len(tmp_y_texts) != 0 : tmp_y_replys.append(tmp_y_texts)\n",
        "                            tmp_X_texts = []\n",
        "                            tmp_y_texts = []\n",
        "\n",
        "                    if AI_TARGET_NAME in text[1:text.index(\"]\")] :\n",
        "                        ptext = text[(text.rfind(']') + 2) : len(text)]\n",
        "                        if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                            before_respondent = AI_TARGET_NAME\n",
        "                            if ptext != '' : tmp_y_texts.append(ptext)\n",
        "                    elif \"]\" in text :\n",
        "                        ptext = text[(text.rfind(']') + 2) : len(text)]\n",
        "                        if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                            before_respondent = cur_respondent\n",
        "                            if ptext != '' : tmp_X_texts.append(ptext)\n",
        "\n",
        "                    if idx == len(texts) - 1 :\n",
        "                        if len(tmp_X_texts) != 0 : tmp_X_replys.append(tmp_X_texts)\n",
        "                        if len(tmp_y_texts) != 0 : tmp_y_replys.append(tmp_y_texts)\n",
        "\n",
        "            if len(tmp_X_replys) != 0 : tmp_X_respondents.append(tmp_X_replys)\n",
        "            if len(tmp_y_replys) != 0 : tmp_y_respondents.append(tmp_y_replys)\n",
        "\n",
        "        if len(tmp_X_respondents) != 0 : X_dataset.append(tmp_X_respondents)\n",
        "        if len(tmp_y_respondents) != 0 : y_dataset.append(tmp_y_respondents)\n",
        "\n",
        "# Functions that init reply's data type is list. so, change data type to str by this function\n",
        "def unzip_replys(X_dataset, y_dataset) :\n",
        "    # If don't use list, data type is str. this is caused by need just one word on word2vec\n",
        "    for idx, respondent in enumerate(X_dataset) : \n",
        "        for i, reply in enumerate(respondent) :\n",
        "            X_dataset[idx][i] = X_dataset[idx][i][0][0].split()\n",
        "\n",
        "    for idx, respondent in enumerate(y_dataset) : \n",
        "        for i, reply in enumerate(respondent) :\n",
        "            y_dataset[idx][i] = y_dataset[idx][i][0][0].split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smqf-vLlU03k"
      },
      "source": [
        "Data Pre-processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMp3NxIaU1Cp"
      },
      "source": [
        "# Functions that auto calibrate spacing functions\n",
        "def auto_spacing(X_dataset, y_dataset) :\n",
        "    calibrate_spcaing(X_dataset)\n",
        "    calibrate_spcaing(y_dataset)\n",
        "\n",
        "def calibrate_spcaing(context_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    for order, respondent in enumerate(context_dataset) :\n",
        "        for idx, texts in enumerate(respondent) :\n",
        "            for repairIdx, text in enumerate(texts) :\n",
        "                analyzedRes = MECAB.pos(text)\n",
        "                for mecabR in analyzedRes :\n",
        "                    # \"MAG\" == Adverb\n",
        "                    if mecabR[1] == \"MAG\" :\n",
        "                        try :\n",
        "                            startIdx = text.index(mecabR[0])\n",
        "                            if len(mecabR[0]) == 1 :\n",
        "                                if text[startIdx + 1] != \" \" and text[startIdx + 1].isalnum() : \n",
        "                                    if text[startIdx + 2] != \" \" :\n",
        "                                        text = text[:startIdx] + text[startIdx] + \" \" + text[startIdx + 1 :]\n",
        "                                        context_dataset[order][idx][repairIdx] = text\n",
        "                            else : \n",
        "                                if text[startIdx + len(mecabR[0])] != \" \" and text[startIdx + len(mecabR[0])].isalnum() : \n",
        "                                    if text[startIdx + len(mecabR[0]) + 1] != \" \" :\n",
        "                                        text = text[:startIdx] + text[startIdx : startIdx + len(mecabR[0])] + \" \" + text[startIdx + len(mecabR[0]) :]\n",
        "                                        context_dataset[order][idx][repairIdx] = text\n",
        "                        except IndexError :\n",
        "                            continue\n",
        "\n",
        "# Functions that delete stopword in sentence\n",
        "def stopword_filtering(X_dataset, y_dataset) :\n",
        "    delete_stopword(X_dataset)\n",
        "    delete_stopword(y_dataset)\n",
        "\n",
        "def delete_stopword(word_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','하다']\n",
        "    for order, respondent in enumerate(word_dataset) :\n",
        "        for idx, reply in enumerate(respondent) :\n",
        "            for i, text in enumerate(reply) :\n",
        "                tokenized_data = MECAB.morphs(text)\n",
        "                tokenized_data = [word for word in tokenized_data if not word in stop_words]\n",
        "                result_text = ''.join(tokenized_data)\n",
        "                if result_text != '' : word_dataset[order][idx][i] = result_text\n",
        "\n",
        "# Functions that divide X, y dataset in learning set\n",
        "def combine_dataset(X_dataset, y_dataset) :\n",
        "    global OOT\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "    for idx in range(0, DN) :\n",
        "        tmp_X = np.empty(DN)\n",
        "        tmp_y = np.empty(DN)\n",
        "        X_size = len(X_dataset[idx])\n",
        "        y_size = len(y_dataset[idx])\n",
        "\n",
        "        if X_size > y_size : \n",
        "            tmp_X = X_dataset[idx][1 : y_size + 1].flatten()\n",
        "            tmp_y = y_dataset[idx].flatten()\n",
        "        elif y_size > X_size : \n",
        "            tmp_X = X_dataset[idx].flatten()\n",
        "            tmp_y = y_dataset[idx][1 : X_size + 1].flatten()\n",
        "        else :\n",
        "            tmp_X = X_dataset[idx].flatten()\n",
        "            tmp_y = y_dataset[idx].flatten()\n",
        "\n",
        "        if idx == 0 :\n",
        "            X.append(tmp_X)\n",
        "            y.append(tmp_y)\n",
        "        elif idx == 1 :\n",
        "            # Type transformation list into ndarray\n",
        "            X = np.r_[X[0], tmp_X]\n",
        "            y = np.r_[y[0], tmp_y]\n",
        "        else :\n",
        "            X = np.r_[X, tmp_X]\n",
        "            y = np.r_[y, tmp_y]\n",
        "\n",
        "    # Set the total number of datasets to be a multiple of 10\n",
        "    if len(X) % 10 != 0 : X = X[0 : (len(X) - (len(X) % 10 ))]\n",
        "    if len(y) % 10 != 0 : y = y[0 : (len(y) - (len(y) % 10 ))]\n",
        "\n",
        "    # Nan check\n",
        "    if np.isnan(X.all()) : print(\"X dataset include NaN data\")\n",
        "    if np.isnan(y.all()) : print(\"y dataset include NaN data\")\n",
        "\n",
        "    # Reshape timestep and vector_length\n",
        "    return X.reshape(int(X.shape[0]/OOT), OOT, 1), y.reshape(int(y.shape[0]/OOT), OOT, 1)\n",
        "\n",
        "# 빈도수 너무 높거나 낮은건 제외하는 메서드 추가 고려 (데이터 학습 제대로 안 되면)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lglCQxR0WGhE"
      },
      "source": [
        "Word2Vec Functions (Distributed Representation)\n",
        "*   [Word2Vec Reference 1](https://ebbnflow.tistory.com/153)\n",
        "*   [Word2Vec Reference 2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)\n",
        "\n",
        "\n",
        "*   [gensim Word2VecKeyedVectors, add](https://github.com/RaRe-Technologies/gensim/issues/2268)\n",
        "\n",
        "\n",
        "*   [PCA(sklearn) Reference](https://m.blog.naver.com/tjdrud1323/221720259834)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaczn0GQ71N4"
      },
      "source": [
        "# Functions that draw a two-dimensional graph by entering the words, values of the two-dimensional X-axis, and values of the Y-axis.\n",
        "def plot_2d_graph(vocabs, xs, ys):\n",
        "    plt.figure(figsize=(8 ,6))\n",
        "    plt.scatter(xs, ys, marker = 'o')\n",
        "    for i, v in enumerate(vocabs):\n",
        "        plt.annotate(v, xy=(xs[i], ys[i]))\n",
        "\n",
        "# Functions that conversioning dataset to word2vec format\n",
        "def dataset_to_word2vec(X_dataset, y_dataset) :\n",
        "    return word2vec(X_dataset, True), word2vec(y_dataset, False)\n",
        "\n",
        "def word2vec(word_dataset, isXdataset) :\n",
        "    global CORPUS, OOT\n",
        "\n",
        "    result = []\n",
        "    for idx, respondent in enumerate(word_dataset) :\n",
        "        # Init words and vectors\n",
        "        w2v = Word2Vec(respondent, size=OOT, window=5, min_count=1, workers=6, sg=1)\n",
        "\n",
        "        # Set word vectors\n",
        "        if isXdataset : \n",
        "            CORPUS.add(entities=list(w2v.wv.vocab.keys()), weights=w2v.wv.vectors, replace=False)\n",
        "        vocabs = CORPUS.vocab.keys()\n",
        "        word_vector_list = [CORPUS[v] for v in vocabs]\n",
        "\n",
        "        # Confirm word similarity\n",
        "        # if \"경호\" in vocabs : print(word_vectors.most_similar(\"경호\"))\n",
        "        # else : print(word_vectors.most_similar(\"대석\"))\n",
        "        \n",
        "        pca = PCA(n_components=OOT)\n",
        "        xys = pca.fit_transform(word_vector_list)\n",
        "        xs = xys[:,0]\n",
        "        ys = xys[:,1]\n",
        "\n",
        "        # Normalization\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(xys)\n",
        "        xys = scaler.transform(xys)\n",
        "\n",
        "        # Change corpus's values to normalized data\n",
        "        if isXdataset : \n",
        "            for idx, key in enumerate(vocabs) : CORPUS[[key]][0] = xys[idx]\n",
        "\n",
        "        # plot_2d_graph(vocabs, xs, ys)\n",
        "        result.append(xys.reshape(-1))\n",
        "\n",
        "    return np.array(result)\n",
        "\n",
        "def word2vec_talk(talk) :\n",
        "    global OOT\n",
        "    \n",
        "    w2v = Word2Vec(talk, size=OOT, window=5, min_count=1, workers=6, sg=1)\n",
        "\n",
        "    # Set word vectors\n",
        "    word_vectors = w2v.wv\n",
        "    vocabs = word_vectors.vocab.keys()\n",
        "    word_vector_list = [word_vectors[v] for v in vocabs]\n",
        "    \n",
        "    word_vector_list = np.asarray(word_vector_list)\n",
        "\n",
        "    # Confirm word similarity\n",
        "    # if \"경호\" in vocabs : print(word_vectors.most_similar(\"경호\"))\n",
        "    # else : print(word_vectors.most_similar(\"대석\"))\n",
        "\n",
        "    return word_vector_list.reshape(-1)\n",
        "\n",
        "# Functions that conversioning data that formed vector to word\n",
        "def vec2word(vec, isreshape=False, isvec=True, beforeWords=[]) :\n",
        "    global MECAB, CORPUS\n",
        "\n",
        "    sentence = \"\"\n",
        "    if isvec :\n",
        "        if isreshape : sentence = sentence + CORPUS.most_similar(positive=[vec.reshape(OOT,)])[0][0]\n",
        "        else : sentence = sentence + CORPUS.most_similar(positive=[vec])[0][0]\n",
        "    else :\n",
        "        word = CORPUS.most_similar(vec)[0][0]\n",
        "\n",
        "        if not word in beforeWords : sentence = sentence + word\n",
        "        else :\n",
        "            for similar in CORPUS.most_similar(vec) :\n",
        "                if not similar[0] in beforeWords :\n",
        "                    sentence = sentence + similar[0]\n",
        "                    break\n",
        "\n",
        "    stop_poses = ['SF', 'SP', 'SS', 'SO', 'NF', 'NV', 'NA', 'EF', 'EC', 'EP']\n",
        "    tokenized_data = MECAB.pos(sentence)\n",
        "    for pos in stop_poses :\n",
        "        if pos in tokenized_data[len(tokenized_data) - 1][1] : return sentence  \n",
        "\n",
        "    if not sentence in beforeWords : beforeWords.append(sentence)\n",
        "    sentence = sentence + ' ' + vec2word(sentence, isreshape=isreshape, isvec=False, beforeWords=beforeWords)\n",
        "    \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzZv5jsvHH-o"
      },
      "source": [
        "Model Learning Functions\n",
        "*   [Keras loss function Reference](https://keras.io/api/losses/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOQuvy5EHIGL"
      },
      "source": [
        "# Functions that make up the model\n",
        "def modeling_model() :\n",
        "    global model\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same', input_shape=(OOT, 1)))\n",
        "    model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation=\"sigmoid\")) # Performance improves once it passes through layers that have been amplified datas\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(OOT,activation=\"softmax\")) # output data == n( == y_dataset.shape[1]) things\n",
        "    model.compile(\n",
        "        loss=losses.MeanSquaredError(),\n",
        "        optimizer=Adam(learning_rate=0.00001), # pram ex. learning_rate=0.0001\n",
        "        metrics=[metrics.MeanSquaredError()]\n",
        "    )\n",
        "\n",
        "# Function to learn the configured model with the training datas\n",
        "def training_model(X_train, y_train, X_val, y_val) :\n",
        "    global model\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "          validation_data = (X_val, y_val),\n",
        "          batch_size = 20,\n",
        "          epochs = 320,\n",
        "          verbose = 1, # 0 = silent, 1 = progress bar,  2 = one line per epoch.\n",
        "          )\n",
        "    \n",
        "    return history\n",
        "    \n",
        "# Function to evaluate the learned model\n",
        "def evaluating_model(X_test, y_test, history) :\n",
        "    global model\n",
        "\n",
        "    # loss and acc graph (train and val)\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "    history_df[[\"loss\", \"val_loss\"]].plot()\n",
        "\n",
        "    # Acc and Loss about real data\n",
        "    learning_lost, learning_err = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print(\"Learning error % :\", learning_err * 100)\n",
        "    print(\"Learning loss % :\", learning_lost * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxjmmKaVlO9p"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea9DIOdiQ_0o"
      },
      "source": [
        "Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcHWaWjaREpT"
      },
      "source": [
        "# uploaded_files = files.upload()\n",
        "\n",
        "file_list = os.listdir('./')\n",
        "fs = [f for f in file_list if f.endswith('.txt')]\n",
        "\n",
        "talk_list = []\n",
        "DN = 0\n",
        "for f in fs :\n",
        "    \"\"\" \n",
        "    ERROR 3. ParserError: Error tokenizing data. C error \n",
        "    - Solution: add code in read_csv = , sep='\\t'\n",
        "    \"\"\"\n",
        "    talk_list.append(pd.read_csv(f, sep='\\t'))\n",
        "    DN += 1\n",
        "\n",
        "# Init Dataset\n",
        "X_dataset = []\n",
        "y_dataset = []\n",
        "\n",
        "# Data extraction\n",
        "extract_data(X_dataset, y_dataset, talk_list)\n",
        "\n",
        "# Early version is used y_list[][0] and x_list[][0] (1 word) by learning model\n",
        "unzip_replys(X_dataset, y_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG9R5wKTSNwi"
      },
      "source": [
        "Pre-processing Data\n",
        "*  [Dataset Classification Reference 1](https://ganghee-lee.tistory.com/38)\n",
        "*  [Dataset Classification Reference 2](https://ysyblog.tistory.com/69)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HENnALQjSN3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa81635f-96ea-4588-a666-bc13abda1cc9"
      },
      "source": [
        "# Space Modification\n",
        "auto_spacing(X_dataset, y_dataset)\n",
        "\n",
        "# Stopword deleted\n",
        "stopword_filtering(X_dataset, y_dataset)\n",
        "\n",
        "# Data type change on np.ndarray\n",
        "X_dataset = np.asarray(X_dataset).astype(object)\n",
        "y_dataset = np.asarray(y_dataset).astype(object)\n",
        "\n",
        "# Word2Vec on dataset\n",
        "X_dataset, y_dataset = dataset_to_word2vec(X_dataset, y_dataset)\n",
        "\n",
        "# Set learning dataset\n",
        "X_dataset, y_dataset = combine_dataset(X_dataset, y_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5KUvpUqJjwe"
      },
      "source": [
        "Model Training\n",
        "*   [NLP Reference](https://towardsdatascience.com/natural-language-processing-a-crash-course-73d7a07c240c)\n",
        "*   [Issue: Model predictional result is always same](https://github.com/keras-team/keras/issues/6447)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67l_qIoJJjVS"
      },
      "source": [
        "# Data division (Train : Test = 8 : 2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, test_size = 0.2)\n",
        "print(\"########## Train + Validation (X,) (y,) / Test (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "# Data division (Train : Validation = 8 : 2)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)\n",
        "print(\"########## Train (X,) (y,) / Validation (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "print('\\n')\n",
        "if os.path.isfile('Aengmu.h5') : \n",
        "    # Load model\n",
        "    print('#################### Model Load... ####################')\n",
        "    model = tf.keras.models.load_model('Aengmu.h5')\n",
        "else :\n",
        "    print('#################### Init Model... ####################')\n",
        "    # Laerning model\n",
        "    model = Sequential()\n",
        "    # Modeling\n",
        "    modeling_model()\n",
        "    \n",
        "# Training\n",
        "history = training_model(X_train, y_train, X_val, y_val)\n",
        "# Evaluating\n",
        "evaluating_model(X_test, y_test, history)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA4mqS5DTpn5"
      },
      "source": [
        "Prediction (Talk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3zCKMclQz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc6daaa-a51a-4b65-c7ce-0bec3849da71"
      },
      "source": [
        "while True :\n",
        "    try :\n",
        "        talk = input(\"앵무에게 보낼 카톡: \")\n",
        "\n",
        "        if talk == \"\" : \n",
        "            print(\"대화 종료\")\n",
        "            break\n",
        "\n",
        "        talk_vec = word2vec_talk([talk])\n",
        "        talk_vec = talk_vec.reshape(int(talk_vec.shape[0]/OOT), OOT, 1)\n",
        "        Aengmu = model.predict(talk_vec)\n",
        "\n",
        "        print('앵무: ' + vec2word(Aengmu[len(Aengmu) - 1]) + '\\n')\n",
        "    except ValueError :\n",
        "        print(\"ERROR, code 00001\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "앵무에게 보낼 카톡: this is test\n",
            "앵무: 생각 만들고\n",
            "\n",
            "앵무에게 보낼 카톡: \n",
            "대화 종료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0_MpEIFjCOK"
      },
      "source": [
        "Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JpataRYhQAr"
      },
      "source": [
        "#model.save('Aengmu.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}