{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aengmu_v2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4wGf2gI5Lao4buRp25zAE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaeSeokSong/Aengmu/blob/main/Aengmu_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtLYgO_OxLaU"
      },
      "source": [
        "# [ERROR 4] Korean(Hangul) breaking phenomenon Solution on Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce18aOPbxLkY"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 4. plot \"Korean\" breaking phenomenon\n",
        "- Solution: Installing(↓) and setting(plt.rc('font', family='NanumBarunGothic')) Nanum font, after runtime restart\n",
        "\"\"\"\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96hYYUFzcaxY"
      },
      "source": [
        "# Google Drive Local Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHqDHk2ucc7M"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgHeq1Zcb7M2"
      },
      "source": [
        "# Install Morpheme analyzer\n",
        "*   [Reference](https://soohee410.github.io/compare_tagger)\n",
        "*   [Install](https://sanghyu.tistory.com/170)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtYcWqVsb5Xk"
      },
      "source": [
        "# okt, komoran, kkma\n",
        "# install konlpy (okt, komoran, kkma)\n",
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy\n",
        "\n",
        "# mecab (take a long time)\n",
        "# set env\n",
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# install konlpy (mecab)\n",
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GobPxVERuegZ"
      },
      "source": [
        "# Error Solution\n",
        "\n",
        "1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings' [(JVM)](https://gyulogs.tistory.com/130)\n",
        "2. NameError: name 'Tagger' is not defined [(Mecab)](https://sosomemo.tistory.com/31)\n",
        "3. ParserError: Error tokenizing data. C error [(Pandas)](https://mskim8717.tistory.com/82)\n",
        "4. plot \"Korean\" breaking phenomenon on Colab [(Matplotlib)](https://teddylee777.github.io/colab/colab-korean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpM2dDEuuesD"
      },
      "source": [
        "\"\"\" \n",
        "ERROR 1. TypeError: startJVM() got an unexpected keyword argument 'convertStrings'\n",
        "- Solution: /usr/local/lib/python3.7/dist-packages/konlpy/jvm.py, 67 line (convertStrings=True) comments processing and save jvm.py before import pakage\n",
        "\"\"\"\n",
        "\n",
        "\"\"\" \n",
        "ERROR 2. NameError: name 'Tagger' is not defined \n",
        "- Solution: Execute mecab.sh script (under code excute)\n",
        "\"\"\"\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu59D-1sc93c"
      },
      "source": [
        "# Import\n",
        "*   [KoNLPy Reperence](https://konlpy-ko.readthedocs.io/ko/v0.4.3/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRaXnlVOWpot"
      },
      "source": [
        "# import for MechineLearning\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import metrics, losses, callbacks\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# import Morpheme analyzer\n",
        "from konlpy.tag import Kkma, Komoran, Okt, Mecab\n",
        "from konlpy.utils import pprint\n",
        "\n",
        "# import etc\n",
        "from google.colab import files\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Change run location\n",
        "%cd /content/drive/My Drive/DeepLearning/PROJECT_AON\n",
        "# Matplotlib set font on NanumBarunGothic\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "# Numpy print set\n",
        "np.set_printoptions(linewidth=200, precision=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGm7Up9L5Y98"
      },
      "source": [
        "#Grobal Variable\n",
        "*   [Concept Reference](https://reniew.github.io/25/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4l77b8v5aKe"
      },
      "source": [
        "\"\"\"\n",
        "ranking about time spent morpheme analyzing (The fastest is number one.)\n",
        "\n",
        "1. Mecab\n",
        "2. Komoran\n",
        "3. Okt\n",
        "4. Kkma\n",
        "\n",
        "Mecab is faster than Kkma about 30~40 times\n",
        "Mecab is faster than Okt about 10 times\n",
        "Mecab is faster than Komoran about 5 times\n",
        "\"\"\"\n",
        "\n",
        "# Early versions use macab.\n",
        "MECAB = Mecab()\n",
        "\n",
        "# Learning target's name\n",
        "AI_TARGET_NAME = \"대석\"\n",
        "\n",
        "# Word2vec model\n",
        "CORPUS = None\n",
        "\n",
        "# Output size Or Time_step\n",
        "OOT = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtyggGY871Bm"
      },
      "source": [
        "# Funtion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm5-VhoeT8Jx"
      },
      "source": [
        "Data Prepare Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8emd9lpUyXR"
      },
      "source": [
        "# Functions that extract reply time (minute)\n",
        "def extract_replytime(content) :\n",
        "    idx_list1 = [i for i, value in enumerate(content) if value == ']']\n",
        "    idx_list2 = [i for i, value in enumerate(content) if value == '[']\n",
        "\n",
        "    end_idx = idx_list1[1]\n",
        "    start_idx = idx_list2[1] + 1\n",
        "\n",
        "    replytime = content[start_idx : end_idx]\n",
        "\n",
        "    if replytime[:2] == \"오전\" :\n",
        "        replytime = (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "    else :\n",
        "        replytime = 60 * 12 + (int(replytime[2:replytime.index(\":\")]) * 60) + int(replytime[replytime.index(\":\") + 1 :])\n",
        "\n",
        "    return replytime\n",
        "\n",
        "# Functions that comparison about reply time\n",
        "def compare_replytime(pre_text, cur_text) :\n",
        "    pre_time = extract_replytime(pre_text)\n",
        "    cur_time = extract_replytime(cur_text)\n",
        "\n",
        "    result = False\n",
        "    if cur_time - pre_time >= 10 : result = True\n",
        "\n",
        "    return result\n",
        "\n",
        "# Functions that extract Data about kakao talk's content\n",
        "def extract_data(X_dataset, y_dataset, talk_list) :\n",
        "    global AI_TARGET_NAME\n",
        "\n",
        "    for talk in talk_list :\n",
        "        tmp_X_respondents = []\n",
        "        tmp_y_respondents = []\n",
        "        for texts in talk.values :\n",
        "            tmp_X_replys = []\n",
        "            tmp_y_replys = []\n",
        "\n",
        "            tmp_X_texts = []\n",
        "            tmp_y_texts = []\n",
        "\n",
        "            before_respondent = \"\"\n",
        "\n",
        "            texts = texts.tolist()\n",
        "            for idx, text in enumerate(texts) : \n",
        "                # Don't extract email and date\n",
        "                if (not \"저장한 날짜\" in text) and (not \"--------------\" in text) and (not \"@\" in text) :\n",
        "                    if not \"]\" in text : \n",
        "                        if before_respondent == AI_TARGET_NAME : tmp_y_texts.append(text)\n",
        "                        else : tmp_X_texts.append(text)\n",
        "\n",
        "                        continue\n",
        "                    else : cur_respondent = text[1:text.index(\"]\")]\n",
        "\n",
        "                    if (\":\" in text) and (before_respondent != \"\") :\n",
        "                        if compare_replytime(texts[idx - 1], text) or cur_respondent != before_respondent:\n",
        "                            if len(tmp_X_texts) != 0 : tmp_X_replys.append(tmp_X_texts)\n",
        "                            if len(tmp_y_texts) != 0 : tmp_y_replys.append(tmp_y_texts)\n",
        "                            tmp_X_texts = []\n",
        "                            tmp_y_texts = []\n",
        "\n",
        "                    if AI_TARGET_NAME in text[1:text.index(\"]\")] :\n",
        "                        ptext = text[(text.rfind(']') + 2) : len(text)]\n",
        "                        if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                            before_respondent = AI_TARGET_NAME\n",
        "                            if ptext != '' : tmp_y_texts.append(ptext)\n",
        "                    elif \"]\" in text :\n",
        "                        ptext = text[(text.rfind(']') + 2) : len(text)]\n",
        "                        if ptext.find(\"http\") == -1 and ptext != \"사진\" and ptext != \"\" : \n",
        "                            before_respondent = cur_respondent\n",
        "                            if ptext != '' : tmp_X_texts.append(ptext)\n",
        "\n",
        "                    if idx == len(texts) - 1 :\n",
        "                        if len(tmp_X_texts) != 0 : tmp_X_replys.append(tmp_X_texts)\n",
        "                        if len(tmp_y_texts) != 0 : tmp_y_replys.append(tmp_y_texts)\n",
        "\n",
        "            if len(tmp_X_replys) != 0 : tmp_X_respondents.append(tmp_X_replys)\n",
        "            if len(tmp_y_replys) != 0 : tmp_y_respondents.append(tmp_y_replys)\n",
        "\n",
        "        if len(tmp_X_respondents) != 0 : X_dataset.append(tmp_X_respondents)\n",
        "        if len(tmp_y_respondents) != 0 : y_dataset.append(tmp_y_respondents)\n",
        "\n",
        "# Functions that init reply's data type is list. so, change data type to str by this function\n",
        "def unzip_replys(X_dataset, y_dataset) :\n",
        "    # If don't use list, data type is str. this is caused by need just one word on word2vec\n",
        "    for idx, respondent in enumerate(X_dataset) : \n",
        "        for i, reply in enumerate(respondent) :\n",
        "            X_dataset[idx][i] = X_dataset[idx][i][0][0].split()\n",
        "\n",
        "    for idx, respondent in enumerate(y_dataset) : \n",
        "        for i, reply in enumerate(respondent) :\n",
        "            y_dataset[idx][i] = y_dataset[idx][i][0][0].split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smqf-vLlU03k"
      },
      "source": [
        "Data Pre-processing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMp3NxIaU1Cp"
      },
      "source": [
        "# Functions that auto calibrate spacing functions\n",
        "def auto_spacing(X_dataset, y_dataset) :\n",
        "    calibrate_spcaing(X_dataset)\n",
        "    calibrate_spcaing(y_dataset)\n",
        "\n",
        "def calibrate_spcaing(context_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    for order, respondent in enumerate(context_dataset) :\n",
        "        for idx, texts in enumerate(respondent) :\n",
        "            for repairIdx, text in enumerate(texts) :\n",
        "                analyzedRes = MECAB.pos(text)\n",
        "                for mecabR in analyzedRes :\n",
        "                    # \"MAG\" == Adverb\n",
        "                    if mecabR[1] == \"MAG\" :\n",
        "                        try :\n",
        "                            startIdx = text.index(mecabR[0])\n",
        "                            if len(mecabR[0]) == 1 :\n",
        "                                if text[startIdx + 1] != \" \" and text[startIdx + 1].isalnum() : \n",
        "                                    if text[startIdx + 2] != \" \" :\n",
        "                                        text = text[:startIdx] + text[startIdx] + \" \" + text[startIdx + 1 :]\n",
        "                                        context_dataset[order][idx][repairIdx] = text\n",
        "                            else : \n",
        "                                if text[startIdx + len(mecabR[0])] != \" \" and text[startIdx + len(mecabR[0])].isalnum() : \n",
        "                                    if text[startIdx + len(mecabR[0]) + 1] != \" \" :\n",
        "                                        text = text[:startIdx] + text[startIdx : startIdx + len(mecabR[0])] + \" \" + text[startIdx + len(mecabR[0]) :]\n",
        "                                        context_dataset[order][idx][repairIdx] = text\n",
        "                        except IndexError :\n",
        "                            continue\n",
        "\n",
        "# Functions that delete stopword in sentence\n",
        "def stopword_filtering(X_dataset, y_dataset) :\n",
        "    delete_stopword(X_dataset)\n",
        "    delete_stopword(y_dataset)\n",
        "\n",
        "def delete_stopword(word_dataset) :\n",
        "    global MECAB\n",
        "\n",
        "    stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','하다']\n",
        "    for order, respondent in enumerate(word_dataset) :\n",
        "        for idx, reply in enumerate(respondent) :\n",
        "            for i, text in enumerate(reply) :\n",
        "                tokenized_data = MECAB.morphs(text)\n",
        "                tokenized_data = [word for word in tokenized_data if not word in stopwords]\n",
        "                result_text = ''.join(tokenized_data)\n",
        "                if result_text != '' : word_dataset[order][idx][i] = result_text\n",
        "\n",
        "# Functions that divide X, y dataset in learning set\n",
        "def combine_dataset(X_dataset, y_dataset) :\n",
        "    X = []\n",
        "    y = []\n",
        "    for idx in range(0, len(X_dataset)) :\n",
        "\n",
        "        if len(X_dataset[idx]) > len(y_dataset[idx]) : X_dataset[idx] = X_dataset[idx][1 : len(y_dataset[idx]) + 1]\n",
        "        elif len(y_dataset[idx]) > len(X_dataset[idx]) : y_dataset[idx] = y_dataset[idx][1 : len(X_dataset[idx]) + 1]\n",
        "\n",
        "        if idx == 0 :\n",
        "            X.append(X_dataset[idx].reshape(X_dataset[idx].shape[0], 1, 1))\n",
        "            y.append(y_dataset[idx].reshape(y_dataset[idx].shape[0], 1, 1))\n",
        "        elif idx == 1 :\n",
        "            X = np.r_[X[0], X_dataset[idx].reshape(X_dataset[idx].shape[0], 1, 1)]\n",
        "            y = np.r_[y[0], y_dataset[idx].reshape(y_dataset[idx].shape[0], 1, 1)]\n",
        "        else :\n",
        "            X = np.r_[X, X_dataset[idx].reshape(X_dataset[idx].shape[0], 1, 1)]\n",
        "            y = np.r_[y, y_dataset[idx].reshape(y_dataset[idx].shape[0], 1, 1)]\n",
        "\n",
        "    # Set the total number of datasets to be a multiple of 10\n",
        "    if len(X) % 10 != 0 : X = X[0 : (len(X) - (len(X) % 10 ))]\n",
        "    if len(y) % 10 != 0 : y = y[0 : (len(y) - (len(y) % 10 ))]\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Functions that factorization for reshaping's variable (just one)\n",
        "def factorization(x) :\n",
        "    d = 2 \n",
        "    \n",
        "    while d <= x : \n",
        "        if x % d == 0 : break\n",
        "        else : d = d + 1\n",
        "    \n",
        "    return d\n",
        "\n",
        "def set_output_size(dataset) :\n",
        "    for respondent in dataset :\n",
        "        print(len(respondent))\n",
        "\n",
        "\"\"\"\n",
        "# Functions that delete duplication about data\n",
        "def delete_duplication(X_dataset, y_dataset) :\n",
        "    return cutting_duplication(X_dataset), cutting_duplication(y_dataset)\n",
        "\n",
        "def cutting_duplication(dataset) :\n",
        "    for idx, respondent in enumerate(dataset) :\n",
        "        for i, reply in enumerate(respondent) :\n",
        "            dataset[idx][i] = list(set(reply))\n",
        "\"\"\"\n",
        "\n",
        "# 빈도수 너무 높거나 낮은건 제외하는 메서드 추가해야됌 (다음에 할 거)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lglCQxR0WGhE"
      },
      "source": [
        "Word2Vec Functions (Distributed Representation)\n",
        "*   [Word2Vec Reference 1](https://ebbnflow.tistory.com/153)\n",
        "*   [Word2Vec Reference 2](https://monetd.github.io/python/nlp/Word-Embedding-Word2Vec-%EC%8B%A4%EC%8A%B5/#%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec-%EB%A7%8C%EB%93%A4%EA%B8%B0)\n",
        "*   [PCA(sklearn) Reference](https://m.blog.naver.com/tjdrud1323/221720259834)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaczn0GQ71N4"
      },
      "source": [
        "# Functions that draw a two-dimensional graph by entering the words, values of the two-dimensional X-axis, and values of the Y-axis.\n",
        "def plot_2d_graph(vocabs, xs, ys):\n",
        "    plt.figure(figsize=(8 ,6))\n",
        "    plt.scatter(xs, ys, marker = 'o')\n",
        "    for i, v in enumerate(vocabs):\n",
        "        plt.annotate(v, xy=(xs[i], ys[i]))\n",
        "\n",
        "# Functions that conversioning dataset to word2vec format\n",
        "def dataset_to_word2vec(X_dataset, y_dataset) :\n",
        "    return word2vec(X_dataset), word2vec(y_dataset)\n",
        "\n",
        "def word2vec(word_dataset) :\n",
        "    global CORPUS\n",
        "\n",
        "    result = []\n",
        "    for respondent in word_dataset :\n",
        "        # Init words and vectors\n",
        "        w2v = Word2Vec(respondent, size=OOT, window=3, min_count=1, workers=6, sg=1)\n",
        "\n",
        "        # Set word vectors\n",
        "        CORPUS = w2v.wv\n",
        "        vocabs = CORPUS.vocab.keys()\n",
        "        word_vector_list = [CORPUS[v] for v in vocabs]\n",
        "\n",
        "        # Confirm word similarity\n",
        "        # if \"경호\" in vocabs : print(word_vectors.most_similar(\"경호\"))\n",
        "        # else : print(word_vectors.most_similar(\"대석\"))\n",
        "        \n",
        "        pca = PCA(n_components=2)\n",
        "        xys = pca.fit_transform(word_vector_list)\n",
        "        xs = xys[:,0]\n",
        "        ys = xys[:,1]\n",
        "\n",
        "        # Normalization\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(xys)\n",
        "        xys = scaler.transform(xys)\n",
        "\n",
        "        # plot_2d_graph(vocabs, xs, ys)\n",
        "        result.append(xys.reshape(-1))\n",
        "\n",
        "    return np.array(result)\n",
        "\n",
        "# vec2word 함수 만들어야 함 (다다음에 할 거)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzZv5jsvHH-o"
      },
      "source": [
        "Model Learning Functions\n",
        "*   [Keras loss function Reference](https://keras.io/api/losses/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOQuvy5EHIGL"
      },
      "source": [
        "# Functions that make up the model\n",
        "def modeling_model() :\n",
        "    global model\n",
        "\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same', input_shape=(OOT, 1)))\n",
        "    model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same'))\n",
        "    model.add(MaxPooling1D(pool_size=3, padding='same'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation=\"sigmoid\")) # Performance improves once it passes through layers that have been amplified datas\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(OOT, activation=\"softmax\")) # output data == n( == y_dataset.shape[1]) things\n",
        "    model.compile(\n",
        "        loss=losses.MeanSquaredError(), # loss function for NLP is categorical_crossentropy(keras.losses) << personally\n",
        "        optimizer=Adam(), # pram ex. learning_rate=0.0001\n",
        "        metrics=['mean_squared_error'] # accuracy for NLP is categorical_accuracy(keras.metrics) << personally\n",
        "    )\n",
        "\n",
        "# Function to learn the configured model with the training datas\n",
        "def training_model(X_train, y_train, X_val, y_val) :\n",
        "    global model\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "          validation_data = (X_val, y_val),\n",
        "          batch_size = 256, # Big == good (but enough)\n",
        "          epochs = 32,\n",
        "          verbose = 2,\n",
        "          )\n",
        "    \n",
        "    return history\n",
        "    \n",
        "# Function to evaluate the learned model\n",
        "def evaluating_model(X_test, y_test, history) :\n",
        "    global model\n",
        "\n",
        "    # loss and acc graph (train and val)\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "    history_df[[\"loss\", \"val_loss\"]].plot()\n",
        "\n",
        "    # Acc and Loss about real data\n",
        "    learning_lost, learning_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print(\"Learning accuracy :\", learning_acc)\n",
        "    print(\"Learning loss % :\", learning_lost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxjmmKaVlO9p"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea9DIOdiQ_0o"
      },
      "source": [
        "Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcHWaWjaREpT"
      },
      "source": [
        "# uploaded_files = files.upload()\n",
        "\n",
        "file_list = os.listdir('./')\n",
        "fs = [f for f in file_list if f.endswith('.txt')]\n",
        "\n",
        "talk_list = []\n",
        "for f in fs :\n",
        "    \"\"\" \n",
        "    ERROR 3. ParserError: Error tokenizing data. C error \n",
        "    - Solution: add code in read_csv = , sep='\\t'\n",
        "    \"\"\"\n",
        "    talk_list.append(pd.read_csv(f, sep='\\t'))\n",
        "\n",
        "# Init Dataset\n",
        "X_dataset = []\n",
        "y_dataset = []\n",
        "\n",
        "# Data extraction\n",
        "extract_data(X_dataset, y_dataset, talk_list)\n",
        "\n",
        "# Early version is used y_list[][0] and x_list[][0] (1 word) by learning model\n",
        "unzip_replys(X_dataset, y_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG9R5wKTSNwi"
      },
      "source": [
        "Pre-processing Data\n",
        "*  [Dataset Classification Reference 1](https://ganghee-lee.tistory.com/38)\n",
        "*  [Dataset Classification Reference 2](https://ysyblog.tistory.com/69)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HENnALQjSN3l"
      },
      "source": [
        "# Space Modification\n",
        "auto_spacing(X_dataset, y_dataset)\n",
        "\n",
        "# Stopword deleted\n",
        "stopword_filtering(X_dataset, y_dataset)\n",
        "\n",
        "# Data type change on np.ndarray\n",
        "X_dataset = np.asarray(X_dataset).astype(object)\n",
        "y_dataset = np.asarray(y_dataset).astype(object)\n",
        "\n",
        "# Word2Vec on dataset\n",
        "X_dataset, y_dataset = dataset_to_word2vec(X_dataset, y_dataset)\n",
        "\n",
        "# Set learning dataset\n",
        "X_dataset, y_dataset = combine_dataset(X_dataset, y_dataset)\n",
        "\n",
        "# Reshape (word2vec 함수의 size에 맞게 time_step(두번째 인수)를 맞추고 model의 마지막 Dense레이어의 아웃풋 결과값도 동일한 size로 맞추는 중)\n",
        "X_dataset = X_dataset.reshape(int(X_dataset.shape[0]/OOT), OOT, 1)\n",
        "y_dataset = y_dataset.reshape(int(y_dataset.shape[0]/OOT), OOT, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5KUvpUqJjwe"
      },
      "source": [
        "Model Training\n",
        "*   [NLP Reference](https://towardsdatascience.com/natural-language-processing-a-crash-course-73d7a07c240c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67l_qIoJJjVS"
      },
      "source": [
        "# Data division (Train : Validation : Test = 6 : 2 : 2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset,\n",
        "                                                    test_size = 0.2)\n",
        "print(\"########## Train + Validation (X,) (y,) / Test (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
        "                                                    test_size = 0.2)\n",
        "print(\"########## Train (X,) (y,) / Validation (X,) (y,) ##########\")\n",
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "# Laerning model\n",
        "model = Sequential()\n",
        "# Modeling\n",
        "modeling_model()\n",
        "# Training\n",
        "history = training_model(X_train, y_train, X_val, y_val)\n",
        "# Evaluating\n",
        "evaluating_model(X_test, y_test, history)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA4mqS5DTpn5"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk3zCKMclQz2"
      },
      "source": [
        "global CORPUS\n",
        "\n",
        "Aengmu = model.predict(X_test)\n",
        "\n",
        "print(CORPUS.most_similar(positive=[X_test[0].reshape(OOT,)])[0])\n",
        "print(CORPUS.most_similar(positive=[Aengmu[0]])[0])\n",
        "print(CORPUS.most_similar(positive=[y_test[0].reshape(OOT,)])[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}